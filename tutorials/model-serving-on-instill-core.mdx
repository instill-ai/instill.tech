---
title: "Serving Custom Models in Instill Core"
lang: "en-US"
draft: false
description: "Learn how to create and deploy models locally with üîÆ Instill Core using the ‚öóÔ∏è Instill Model MLOps platform."
aiTask: "TextGeneration"


publishedOn: "2024-05-29T12:00:00"
placeholderColor: "bg-instillRed90"
themeImgSrc: "/tutorial-assets/model-serving-on-instill-core/cover-static.png"
themeImgThumbnailSrc: "/tutorial-assets/model-serving-on-instill-core/cover-static.png"
useCase: "Custom Model Serving"
author: "George Strong"
authorAvatarSrc: "/author-avatar/george.jpeg"
authorGitHubUrl: "https://github.com/georgewilliamstrong"
keywords: ["model serving", "MLOps", "Instill Core"]
---

# Unlock new Possibilities with ‚öóÔ∏è Instill Model

**‚öóÔ∏è Instill Model** is a sophisticated MLOps/LLMOps platform specially designed
to orchestrate model serving and monitoring to ensure consistent and reliable
performance. It allows for efficient management and deployment of deep learning
models for unstructured data ETL, and is used to deploy and serve models locally
with **üîÆ Instill Core**, and on the cloud with **‚òÅÔ∏è Instill Cloud**.

## Why use ‚öóÔ∏è Instill Model?

1. Seamless Integration with **üíß Instill VDP**: integrate effortlessly with our
   Versatile Data Pipeline, allowing for streamlined unstructured data ETL and
   model serving workflows.
2. No Code Console Builder: Easily utilize custom models defined with **‚öóÔ∏è
   Instill Model** as modular AI components via **üì∫ Instill Console**, allowing
   for seamless integration into downstream tasks.
3. AutoML Feature (Coming Soon): With the upcoming AutoML feature, **‚öóÔ∏è Instill
   Model** will soon be capable of automating model training and tuning,
   simplifying model optimization for deployment.

This step-by-step tutorial will guide you through the process of setting up your
own custom model with **Instill Model** for local deployment with **üîÆ Instill
Core**.

## Prerequisites

1. **Docker**: Both **üîÆ Instill Core** and **‚öóÔ∏è Instill Model** use **Docker**
   to ensure that models and code can be deployed in consistent, isolated and
   reproducible environments. Please ensure that you have Docker installed and
   running by following the [official
   instructions](https://docs.docker.com/get-docker/), and see our [deployment
   guide](/docs/core/deployment) for recommended resource settings.
2. **‚å®Ô∏è Instill CLI**: The easiest way to launch **üîÆ Instill Core** for local
   deployment is via the **‚å®Ô∏è Instill CLI**. To install **‚å®Ô∏è Instill CLI** using
   Homebrew, please run the following command in your terminal:
   ```bash
   brew install instill-ai/tap/inst
   ```
3. **Launch üîÆ Instill Core**: To launch simply run:
   ```bash
   inst local deploy
   ```
   Please note that the initial launch process may take up to 1 hour, depending
   on your internet speed. Subsequent launches will be much faster, usually
   completing in under 5 minutes.
4. Now that **üîÆ Instill Core** has been deployed, you can access **üì∫ Instill
   Console** at [http://localhost:3000](http://localhost:3000). Please use the
   following initial login details to initiate the password reset process for
   onboarding:

    - Username: `admin`
    - Password: `password`

For further details about launching **üîÆ Instill Core**, we recommend that you
refer to the [deployment guide](/docs/core/deployment) which covers alternative
ways you can launch with Docker Compose or Kubernetes with Helm.

Finally, please note that this guide assumes you have a basic understanding of
machine learning and can code in Python. If you are new to these concepts, we
recommend that you look at our [quick start
guide](/docs/latest/quickstart#authorisation?utm_source=tutorial&utm_medium=link&utm_campaign=model-oss-llm)
which introduces our no-code **üíß Instill VDP** pipeline builder, and also take
a look at some of our other [tutorials](https://www.instill.tech/tutorials).

# Step-by-Step Tutorial

## Step 1: Create a Model Namespace

To get started, navigate to the Model page in the Console window and click the
**+ Create Model** button.

This should bring up a configuration window (see image below) where you are able
to configure your model settings. For a full description of the available
fields, please refer to the [Create a Model](/docs/model/create) page.

In this tutorial, we will be walking through how to create and deploy a version
of the
[TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
model. To follow along, please fill in the configuration fields as per the image
below.

<ZoomableImg src="/tutorial-assets/model-serving-on-instill-core/T1.png"
alt="Configure Model Settings" />

You have now created an empty model namespace on **‚öóÔ∏è Instill Model**. In the
next sections of this tutorial we will show you how to define your own custom
model for deployment!

## Step 2: Create a Model Config

To prepare a model to be served with **‚öóÔ∏è Instill Model**, you first need to
create your own model directory containing two files - `model.py` and
`instill.yaml`.

To configure the
[TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
model, simply open the `instill.yaml` file and populate it with:

<CH.Code>
```yaml instill.yaml
build:
  gpu: true
  python_version: "3.11"
  python_packages:
     - torch==2.2.1
     - transformers==4.36.2
     - accelerate==0.25.0
repo: USER_ID/MODEL_ID
```
</CH.Code>

Importantly, you must replace `USER_ID` with `admin`, and replace `MODEL_ID` for
`tinyllama` - the same Model ID that was specified in [Step
1](#step-1-create-a-model-namespace).

This file specifies the dependencies required to run the model. We will be
loading these libraries in the next stage where we define our model class!

## Step 3. Write a Model Script

In this step we will create the `model.py` file, which will contain the model
class definition. This will be broken down in three phases to demonstrate the
structure of the model class, and explain the methods it should implement.

### Define the Model Initialization

The first phase involves defining the model class and creating the `__init__`
constructor which is responsible for loading the model. Here we will use
`pipeline()` from the
[transformers](https://huggingface.co/docs/transformers/en/index) library to
directly load in the
[TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
model.

<CH.Code>
```python model.py
import torch
from transformers import pipeline

from instill.helpers.ray_config import instill_deployment, InstillDeployable


@instill_deployment
class TinyLlama:
    def __init__(self):
        self.pipeline = pipeline(
            "text-generation",
            model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
```
</CH.Code>


The `instill.helpers.ray_config` package contains the decorators and deployment
object for the model class, which we will use to convert the model class into a
servable model. These are required to properly define a model class for **‚öóÔ∏è
Instill Model**.

### Define the Model Metadata method

In the second phase, we define the `ModelMetadata` method which is responsible
for communicating the models expected input and output shapes to the backend
service. To easily facilitate this, we can make use of the [Python
SDK](/docs/latest/sdk/python-sdk) through the `instill.helpers` module which
provides a number of functions that can be selected according to the AI task the
model performs.

Here, we recognise that the
[TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
model falls under the [Text Generation
Chat](https://www.instill.tech/docs/model/ai-task#text-generation-chat) AI task,
and so we will make use of the
`construct_text_generation_chat_metadata_response` helper function.

Please refer [here](https://www.instill.tech/docs/model/ai-task) for a full list
of the supported AI tasks.

<CH.Code>
```python model.py
# previous imports defined above

from instill.helpers import construct_text_generation_chat_metadata_response


@instill_deployment
class TinyLlama:
    # __init__ defined in phase 1

    def ModelMetadata(self, req):
        return construct_text_generation_chat_metadata_response(req=req)
```
</CH.Code>

### Implement the Inference Method

In the third phase, we implement the inference method `__call__`, which handles
the trigger request from **‚öóÔ∏è Instill Model**, contains the necessary logic to
run the inference, and constructs the response. We use the `StandardTaskIO`
module to parse the request payload into input parameters, and convert the model
outputs to the appropriate response format.

The `TextGenerationChatInput` class from `instill.helpers.const` is used to
define the input format for the [Text Generation
Chat](https://www.instill.tech/docs/model/ai-task#text-generation-chat) AI task,
and the `construct_text_generation_chat_infer_response` function from
`instill.helpers` is used to format the model output into the appropriate
response format.

<CH.Code>
```python model.py
# previous imports defined above

from instill.helpers.const import TextGenerationChatInput
from instill.helpers.ray_io import StandardTaskIO
from instill.helpers import construct_text_generation_chat_infer_response


@instill_deployment
class TinyLlama:
    # __init__ defined in phase 1

    # ModelMetadata method defined in phase 2

    async def __call__(self, request):
        # parse the request and get the corresponding input for text-generation-chat task
        task_text_generation_chat_input: TextGenerationChatInput = (
            StandardTaskIO.parse_task_text_generation_chat_input(request=request)
        )

        # prepare prompt with chat template
        conv = [
            {
                "role": "system",
                "content": "You are a friendly chatbot",
            },
            {
                "role": "user",
                "content": task_text_generation_chat_input.prompt,
            },
        ]

        prompt = self.pipeline.tokenizer.apply_chat_template(
            conv,
            tokenize=False,
            add_generation_prompt=True,
        )

        # inference
        sequences = self.pipeline(
            prompt,
            max_new_tokens=task_text_generation_chat_input.max_new_tokens,
            do_sample=True,
            temperature=task_text_generation_chat_input.temperature,
            top_k=task_text_generation_chat_input.top_k,
            top_p=0.95,
        )

        # convert the model output into response output
        task_text_generation_chat_output = (
            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)
        )

        return construct_text_generation_chat_infer_response(
            req=request,
            # specify the output dimension
            shape=[1, len(sequences)],
            raw_outputs=[task_text_generation_chat_output],
        )


# now simply declare a global entrypoint for deployment
entrypoint = InstillDeployable(TinyLlama).get_deployment_handle()
```
</CH.Code>

Putting it all together, your `model.py` file should now look like this:

<CH.Code>
```python model.py
import torch
from transformers import pipeline

from instill.helpers.ray_config import instill_deployment, InstillDeployable

from instill.helpers.const import TextGenerationChatInput
from instill.helpers.ray_io import StandardTaskIO
from instill.helpers import (
    construct_text_generation_chat_metadata_response,
    construct_text_generation_chat_infer_response,
)


@instill_deployment
class TinyLlama:
    def __init__(self):
        self.pipeline = pipeline(
            "text-generation",
            model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )

    def ModelMetadata(self, req):
        return construct_text_generation_chat_metadata_response(req=req)

    async def __call__(self, request):
        # parse the request and get the corresponding input for text-generation-chat task
        task_text_generation_chat_input: TextGenerationChatInput = (
            StandardTaskIO.parse_task_text_generation_chat_input(request=request)
        )

        # prepare prompt with chat template
        conv = [
            {
                "role": "system",
                "content": "You are a friendly chatbot",
            },
            {
                "role": "user",
                "content": task_text_generation_chat_input.prompt,
            },
        ]

        prompt = self.pipeline.tokenizer.apply_chat_template(
            conv,
            tokenize=False,
            add_generation_prompt=True,
        )

        # inference
        sequences = self.pipeline(
            prompt,
            max_new_tokens=task_text_generation_chat_input.max_new_tokens,
            do_sample=True,
            temperature=task_text_generation_chat_input.temperature,
            top_k=task_text_generation_chat_input.top_k,
            top_p=0.95,
        )

        # convert the model output into response output
        task_text_generation_chat_output = (
            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)
        )

        return construct_text_generation_chat_infer_response(
            req=request,
            # specify the output dimension
            shape=[1, len(sequences)],
            raw_outputs=[task_text_generation_chat_output],
        )


# now simply declare a global entrypoint for deployment
entrypoint = InstillDeployable(TinyLlama).get_deployment_handle()
```
</CH.Code>

Awesome üòé, you have now defined your own custom model class for model serving
with **‚öóÔ∏è Instill Model**. In the next step, we will show you how to build and
deploy this model locally with **üîÆ Instill Core**!

## Step 4: Build and Deploy the Model

First, you must ensure that you have the same Python version installed in your
local environment as specified in the `instill.yaml` file in [step
2](#step-2-create-a-model-config), in this case `python_version: "3.11"`.

You then need to ensure you have installed the latest version of the [Python
SDK](/docs/latest/sdk/python-sdk) by running:

```bash
pip install instill-sdk
```

### Build the Model Image

You can now build your model image by running the following command from within
the directory containing the `model.py` and `instill.yaml` files:

```bash
instill-build -t v1
```

This command will build the model image under version tag `v1`. Upon successful
completion, you should see a similar output to the following:
```
2024-05-28 01:54:44,404.404 INFO     [Instill Builder] admin/tinyllama:v1 built
2024-05-28 01:54:44,423.423 INFO     [Instill Builder] Done
```

### Push the Model Image

To push the model image to the **üîÆ Instill Core** instance we will need to be
able to login to the hosted Docker registry. To do this, you first need to
create an API token by:
1. Selecting the profile icon in the top right corner of the Console window and
   choosing the Settings option.
2. Select API Tokens from the left-hand menu.
3. Click the **Create Token** button and give it a name, e.g. `tutorial`. Copy
   the generated API token.

<ZoomableImg src="/tutorial-assets/model-serving-on-instill-core/T2.png"
alt="Create API Token" />

Now we can login to the Docker register in the **üîÆ Instill Core** instance by
running:
```bash
docker login localhost:8080
```
and entering the following credentials: - Username: `admin` - Password:
    `API_TOKEN` (replace this with the token you generated in the previous step)

Once logged in, we can push model image `v1` to **‚öóÔ∏è Instill Model** with:

```bash
instill-push -t v1 -u localhost:8080
```

Upon successful completion, you should see a similar output to the following:
```
2024-05-23 23:05:03,484.484 INFO     [Instill Builder] localhost:8080/admin/tinyllama:v1 pushed
2024-05-23 23:05:03,485.485 INFO     [Instill Builder] Done
```

**‚öóÔ∏è Instill Model** will then automatically allocate the resources required by
your model and deploy it. Please note that the deployment time varies based on
the model size and hardware type.

### Status Check

To check the status of your deployed model version you can:
1. Navigate back to the Models page on the Console.
2. Select the `admin/tinyllama` model you created in [Step
   1](#step-1-create-a-model-namespace).
3. Click the **Versions** tab, where you will see the corresponding version ID
or tag of your pushed model image and the `Status` of deployment. You should
then see a similar screen to the image below.

<ZoomableImg src="/tutorial-assets/model-serving-on-instill-core/T3.png" alt="3.
Check Model Status" />

The `Status` will initially show as `Starting`, indicating that your model is
offline and **‚öóÔ∏è Instill Model** is still in the process of allocating resources
and deploying it (this may take a few minutes). Once this status changes to
`Active`, your model is ready to serve requests. üöÄ

## Step 5. Inference

Once your model is deployed and `Active`, you can easily test its behaviour
following these steps:
1. Navigating to the **Overview** tab for your `admin/tinyllama` model.
2. Enter a prompt in the **Input** pane (e.g. What is a rainbow?)
3. Scroll down and hit **Run** to trigger the model inference.

<ZoomableImg src="/tutorial-assets/model-serving-on-instill-core/T4.png" alt="4.
Model Inference" />

You should see a response generated by the
[TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
model in the **Output** pane.

To access model inferences via the API:
1. Navigate to the **API** tab for your `admin/tinyllama` model.
2. Follow the instructions by setting your own `INSTILL_API_TOKEN` as an
   environment variable:
   ```bash
   export INSTILL_API_TOKEN=********
   ```
   and using the provided `curl` command to send a request to the model
   endpoint:
   ```bash
   curl --location 'https://api.instill.tech/model/v1alpha/users/admin/models/tinyllama/trigger' \
   --header 'Content-Type: application/json' \
   --header 'Authorization: Bearer $INSTILL_API_TOKEN' \
   --data '{
     "task_inputs": [
       {
         "text_generation": {
           "prompt": "How is the weather today?",
            "chat_history": [
              {
                "role": "user",
                "content": [
                  {
                    "type": "text",
                    "text": "hi"
                  }
                ]
              }
            ],
            "system_message": "you are a helpful assistant",
            "max_new_tokens": 1024,
            "top_k": 5,
            "temperature": 0.7
          }
        }
      ]
   }'
   ```

## Step 6. Tear Everything Down

After you have finished testing and serving your model, you might want to tear
down the local **üîÆ Instill Core** instance to free up system resources. You can
do this using the **‚å®Ô∏è Instill CLI** command:

```bash
inst local undeploy
```

# Conclusion

Congratulations on successfully deploying and serving a custom model with **‚öóÔ∏è
Instill Model** and **üîÆ Instill Core**! üéâ

By following this tutorial, you've accomplished the following:

1. Set up **üîÆ Instill Core** for local deployment.
2. Created a model namespace and configured model settings.
3. Defined and implemented a custom model class for the
   [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
   model.
4. Built and deployed your model image.
5. Tested your model's inference capabilities via the Console and API.

**‚öóÔ∏è Instill Model** and **üîÆ Instill Core** together provide a powerful,
streamlined solution for managing and deploying your own deep learning models.

Excitingly, you can now connect your own custom models via the **‚öóÔ∏è Instill
Model** [AI component](../docs/component/ai/instill) to construct bespoke **üíß
Instill VDP** pipelines tailored to your unstructured data ETL requirements.
Please see the [Create a Pipeline](../docs/vdp/create) page for more information
on building **üíß Instill VDP** pipelines with **üì∫ Instill Console**.

Ultimately these tools allow you the freedom and creativity to develop and
iterate innovative AI-powered workflows to solve your real-world use cases.

Thank you for following along with this tutorial and stay tuned for more updates
soon! üöÄ
