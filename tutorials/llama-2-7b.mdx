---
title: "The Open Source Journey with Llama2-7B & LLama2-7B-Chat"
lang: "en-US"
draft: false
description: "Step-by-step guidance to implementing Llama2-7B & LLama2-7B-Chat"
sourceConnector: "Null"
destinationConnector: "Null"
aiTask: "TextGeneration"
publishedOn: "2024-03-18T09:30:00"
placeholderColor: "bg-instillRed90"
themeImgSrc: "/tutorial-assets/llama2/"
themeImgThumbnailSrc: "/tutorial-assets/llama2/"
useCase: "Model"
author: "Shih-Chun Huang"
authorAvatarSrc: "/author-avatar/Shih-Chun.jpg"
authorGitHubUrl: "https://github.com/ShihChun-H"
keywords: ["Llama", "Llama2", "Open Source", "Artificial Intelligence", "Generative AI", "Natural Language Processing", "Large Language Model"]
---

# **Llama2**

Launched by Meta AI in 2023, Llama2 represents a suite of both pre-trained and fine-tuned Large Language Models (LLMs), designed for a broad spectrum of Natural Language Processing (NLP) applications. These models, offered freely for both research and commercial purposes, excel in tasks ranging from generating text to coding, showcasing their versatility in handling diverse challenges.

According to the [Llama2 blog](https://llama.meta.com/llama2/) published by Meta, [Llama2](https://github.com/facebookresearch/llama)Â includes model weights and starting code for pre-trained and fine-tuned large language models, ranging from 7B to 70B parameters. Llama2 was trained on 40% more data (2 trillion tokens) than [Llama1](https://github.com/meta-llama/llama/tree/llama_v1), and has double the context length. Llama2 was pre-trained on publicly available online data sources.

<br />

# **Llama2-Chat**

The fine-tuned model, Llama2-Chat, is optimized for dialogue use cases. To ensure safety and helpfulness, Llama2-Chat employs Reinforcement Learning from Human Feedback (RLHF) with over 1 million human annotations, which involves learning from human interactions to improve the model's responses.

# **New in Llama2 (in comparison to Llama1)**

A blog on [Geeky Gadget](https://www.geeky-gadgets.com/llama-1-vs-llama-2/) has made a clear comparison:

### **Llama1**

1. **Model Sizes**: Trained in four sizes: 7, 13, 33, and 65 billion parameters.
2. **Performance**: The 13B parameter model outperformed GPT-3 on most NLP benchmarks, and the largest model was competitive with state-of-the-art models.
3. **Accessibility**: Initially released under a noncommercial license, the weights were later leaked to the public.
4. **Commercial Use**: Access was gated to researchers with restrictions on commercial use.
5. **Parameter Efficiency**: Known for being more parameter-efficient and outperforming larger commercial models like GPT-3.
6. **Open Source Status**: Not mentioned as fully open-source.

### **Llama2**

1. **Model Sizes**: Released in three sizes: 7, 13, and 70 billion parameters, with a potential future release of a 34B parameter model.
2. **Performance**: Outperforms other open-source models in both natural language understanding and head-to-head comparisons.
3. **Accessibility**: All models, including Llama2â€“Chat, are released with weights and are free for many commercial use cases.
4. **Commercial Use**: Available for both research and commercial use.
5. **Training Data**: Trained on 40% more data than Llama1.
6. **Context Length**: Has double the context length compared to Llama1.
7. **Fine-tuning**: Tuned on a large dataset of human preferences (over 1 million annotations) for helpfulness and safety.
8. **Open Source Status**: Described as open source, but disputed by the Open Source Initiative.

### **Key Comparisons**

- **Model Sizes**: Llama2 introduced a model with 70 billion parameters but didnâ€™t include the 33 and 65 billion parameter versions present in Llama1.
- **Performance**: Both generations excelled in performance, with Llama2 building on Llama1â€™s success.
- **Accessibility**: Llama2 expanded accessibility to include commercial use, unlike Llama1â€™s initial noncommercial license.
- **Training and Features**: Llama2 was trained on more data, offered double the context length, and included fine-tuning for helpfulness and safety.
- **Open Source Status**: Llama2 is positioned as an open-source model, though this has been disputed, while Llama1â€™s open-source status was not emphasized.

<br />

# **Implementing Llama2-7B/Llama2-7B-Chat Model with the Instill Model: A Step-by-Step Tutorial**

In this tutorial, you'll learn how to utilize the Llama2-7B/Llama2-7B-Chat model through both Instill Cloud and Instill Core. We'll guide you through the process of making API calls to trigger model inferences and how to set up the model locally for more advanced uses. Whether you're a developer looking to integrate AI capabilities into your project or just interested in exploring what AI can do, this tutorial is designed to provide you with a straightforward path to success.

## **Using Instill Cloud**

Instill Cloud offers an easy way to try out models via API calls. Here's how you can get started:

### **Step 1: Obtain an API Token**

- **Sign in** to Instill Cloud and navigate to the **Settings** section.
- Click on **API Tokens** and **generate a new token**. Make sure to copy this token for later use.

<ZoomableImg
  src="/tutorial-assets/llama2/API-Token.png"
  alt="Where to get API Token"
/>

### **Step 2: Choose Your Model**

On the [API Reference Page](https://openapi.instill.tech/reference/modelpublicservice_triggerorganizationmodel), select the **"Trigger model inference"** option. This tutorial uses Shell as an example, but instructions are available for various programming languages.

<ZoomableImg
  src="/tutorial-assets/llama2/API-reference.png"
  alt="Trigger Model Inference"
/>

### **Step 3: Fill in the Required Fields**

You'll need to provide specific information to make the API call:

- **Authorization**: Enter the token with theÂ **`Bearer`** prefix, e.g.Â **`Bearer`** + **`'YOUR_API_TOKEN'`**(attained from [Instill Cloud](https://instill.tech/)).
- **PATH PARAMS**
    - **organization_model_name**: Use **`organizations/instill-ai/models/llama2-7b`** for Llama2-7B model, or use **`organizations/instill-ai/models/llama2-7b-chat`** for Llama2-7B-Chat model.
- **BODY PARAMS**
    - **task_inputs**: Select **`TEXT_GENERATIONÂ OBJECT `** for Llama2-7B model and **`TEXT_GENERATION_CHATÂ OBJECT `** for Llama2-7B-Chat model.
        - **prompt**: Enter your question for the model.

### **Step 4: Execute the API Call**

After filling in the fields, click **`Try It!`**. You'll receive a **`curl`** command similar to the following, which you can run in your terminal:

Note: We are taking Llama2-7B in this example.

```shellscript
curl --request POST \
     --url https://api.instill.tech/model/v1alpha/organizations%2Finstill-ai%2Fmodels%2Fllama2-7b/trigger \
     --header 'Authorization: Bearer YOUR_API_TOKEN' \
     --header 'accept: application/json' \
     --header 'content-type: application/json' \
     --data '
{
  "task_inputs": [
    {
      "text_generation": {
        "prompt": "Where will GenAI lead mankind?"
      }
    }
  ]
}'
```

## **Using Instill Core**

For those looking to run models locally or on their servers, Instill Core provides a robust solution.

### **Prerequisites**

- Require GPU to run Llama2-7B-Chat model locally
- Both CPU and GPU are compatible for local execution of the Llama2-7b model
- Install Docker on your machine and open Docker and ensure it's running on your machine.
- Have basic knowledge of using terminal or command prompt.

### **Step 1: Setting Up Your Local Environment**

**Launch Instill Model Locally**:

- Clone the Instill Core repository and navigate into the project directory:

```shellscript
$ git clone -b v0.25.0-beta https://github.com/instill-ai/instill-core.git && cd instill-core
```

- Launch all services with:

```shellscript
make all
```

### **Step 2: Adding a Model**

You can add models either through the console at http://localhost:3000/ or via API.

**Via Console**

- Log in with username **admin** and the default password **password** (you can change this later).
- Navigate to "Model Hub" > "Add Model" and enter the model details as found on the [GitHub repository](https://github.com/instill-ai/deprecated-model).

<ZoomableImg
  src="/tutorial-assets/llama2-7b/Add-Model.png"
  alt="How to add model on Console"
/>

**Via API**

- Obtain an API token from the console by navigating to http://localhost:3000/ > **Settings** > **API Tokens**.

<ZoomableImg
  src="/tutorial-assets/llama2-7b/API-Token_core.png"
  alt="Where to get API Token"
/>

- Go to the [API Reference Page](https://openapi.instill.tech/reference/modelpublicservice_createusermodel-1) to create a new model with the following details:
    - **Authorization**: Enter the token with theÂ **`Bearer`** prefix, e.g.Â **`Bearer`** + **`'YOUR_API_TOKEN'`**(attained from http://localhost:3000/).
    - **PATH PARAMS**
        - **user_name**: Use **`users/admin`**.
    - **BODY PARAMS**
        - **id**: Enter an ID for your model.
        - **task**: Select **`TEXT_GENERATIONÂ OBJECT `** for Llama2-7B model and **`TEXT_GENERATION_CHATÂ OBJECT `** for Llama2-7B-Chat model.
        - **model_definition**: Use **`model-definitions/github`**.
        - **configuration**: Provide the model's repository and tag details.
        In this case, it would be
            
            **`repository`**: **`model-llama2-7b-dvc`** for Llama2-7B model and **`model-llama2-7b-chat-dvc`** for Llama2-7B-Chat model.
            
            **`tag`**: **`f16-gpu-transformer-ray-v0.8.0`** for llama2 gpu version
                 **`f32-cpu-transformer-ray-v0.8.0`** for llama2 cpu version
                 **`f16-gpuAuto-transformer-ray-v0.8.0`** for llama2-chat
            

### **Step 3: Triggering Model Inference**

With your model added, you can now trigger model inferences. Here's a **`curl`** command example for model inference:

Note: We are taking Llama2-7B in this example.

```shellscript
curl --location 'http://localhost:8080/model/v1alpha/users/admin/models/llama2-7b/trigger' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer YOUR_API_TOKEN' \
--data '
{
  "task_inputs": [
    {
      "text_generation": {
        "prompt": "Where will GenAI lead mankind?"
      }
    }
  ]
}'
```

By following these steps, you can easily use the Llama2-7B model through Instill Cloud for simple API calls or set it up locally using Instill Core for more extensive testing and development.

<CtaButton
  text="Access Example Pipeline"
  link="https://instill.tech/instill-wombat/pipelines/llama2-7b_vs_llama2-7b-chat"
/>

<br />

# Try it out
<Llama27bVsLlama27bChat />


<br />
<br />


# Showcases

Let's compare the results generated by Llama2-7B and Llama2-7B-Chat!

The primary objective of creating a pipeline to compare Llama2-7B and its fine-tuned counterpart, Llama2-7B-Chat, which is optimized for dialogues, is to meticulously assess their responses across various scenarios. This comparison aims to illuminate the distinct capabilities and suitability of each model for specific use cases. By understanding their differences in performance and applicability, we can make informed decisions about which model to deploy to meet particular requirements effectively.

***

### Reflection

Prompt = `Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Let's think step by step.`

**Llama2-7B OUTPUT**

> A yellow plank of wood with a red arrow pointing upwards.

**Llama2-7B-Chat OUTPUT**

> Explore new horizons: Your next adventure is just a few steps away.

***

### Reading the image

Next up, letâ€™s see if the models can read the text from an image.

Prompt = `What text is written on this image?`

**Llama2-7B OUTPUT**

> The text written on this image is "Difficult Roads Lead to Beautiful Destinations.

**Llama2-7B-Chat OUTPUT**

> The text on the image reads, "DIFFICULT ROADS LEAD TO BEAUTIFUL DESTINATIONS."

***

### Extracting code from an image

Can it extract code from an image? Nowadays, many people post code snippets as images on social media, making it difficult to reuse the code. Perhaps LLaVA can be a solution to this problem.

Prompt = `Extract the code from this image`

**Llama2-7B OUTPUT**

> The image shows a computer screen displaying a code in a programming language. The code consists of multiple lines, with some of them highlighted in yellow. The code appears to be related to a chat application, as it includes phrases such as "instill client" and "text generation chat input." The code also contains information about a company's holiday policy.

**Llama2-7B-Chat OUTPUT**

> Here's the text from the image:
> 
> ```
> import InstantiateClient
> 
> client = InstantiateClient()
> 
> client.input("InstantiateChatInput");
> 
> output = client.get_output()
> 
> if output.text == '':
> print("No output received.")
> else:
> print("InstantiateChatInput:", output.text)
> 
> client.input("InstantiateChatInput")
> 
> output = client.get_output()
> 
> if output.text == '':
> print("No output received.")
> else:
> print("InstantiateChatInput:", output.text)
> 
> client.input("InstantiateChatInput")
> 
> output = client.get_output()
> 
> if output.text == '':
> print("No output received.")
> else:
> print("InstantiateChatInput:", output.text)
> ```
> 
> Please note that without the context of the environment in which this code is being run, it's difficult to provide an accurate interpretation of what the code is doing. Additionally, the `InstantiateClient` module or class is not defined in the code snippet, so it appears to be a placeholder or an assumed module for the purposes of the example code.

***

### Understanding a diagram

Letâ€™s see if it can explain this diagram

Prompt = `Describe the meaning of the diagram`

**Llama2-7B OUTPUT**

> The diagram is a flowchart that represents a business plan. It consists of several arrows and questions, which are meant to guide the reader through the process of creating a business plan. The flowchart starts with a question, "Does it work?", followed by a series of arrows pointing to different directions, indicating various aspects of the business plan. The questions and arrows are intended to help the reader think critically about the plan and make informed decisions. The flowchart is designed to be a visual representation of the business planning process, making it easier to understand and follow.

**Llama2-7B-Chat OUTPUT**

> The diagram depicts a simple decision-making or problem-solving process using a question format that is often associated with the "light bulb" moment of sudden insight. It shows a three-step sequence with a light bulb icon, which represents an idea, and three branches leading from it.
>
> 1. The first step asks, "Does it work?", which suggests testing a new idea or approach.
> 2. If the answer to this question is negative, the second branch leads to the question, "Does it work?", indicating that the same process is repeated if the initial idea does not prove effective.
> 3. If the answer to the second question is no, the third branch leads to "No" and the process ends. This suggests that if the idea does not work after two attempts, it may be concluded that the idea is not viable or effective.
> 4. If the answer to the first question is positive, meaning the initial idea is deemed to work, the process moves to the second step, which asks, "Does it work?" again. This implies that the idea is tested again, perhaps to confirm the result or to refine the idea further.
> 5. If the answer to the second question is still positive, the process leads to the third step, "Yes", and the process ends, indicating that the idea is considered to be effective or successful.
>
> The diagram uses a minimalist approach to convey the concept of iterative problem-solving and the importance of testing and confirming the effectiveness of an idea or approach before implementing it.


<CtaButton
  text="ðŸš€ Try Llama2-7B & Llama2-7B-Chat"
  link="https://instill.tech/instill-wombat/pipelines/llama2-7b_vs_llama2-7b-chat"
/>

<br />
<br />

# Conclusion

