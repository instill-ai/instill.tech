---
title: "Serverless Model Serving on Instill Cloud"
lang: "en-US"
draft: false
description: "Learn how to create, serve, and automatically scale models on ‚òÅÔ∏è Instill Cloud using the ‚öóÔ∏è Instill Model MLOps platform."
aiTask: "TextGeneration"


publishedOn: "2024-05-29T12:00:00"
placeholderColor: "bg-instillRed90"
themeImgSrc: "/tutorial-assets/serverless-model-serving/cover-static.png"
themeImgThumbnailSrc: "/tutorial-assets/serverless-model-serving/cover-static.png"
useCase: "Serverless Model Serving"
author: "George Strong"
authorAvatarSrc: "/author-avatar/george.jpeg"
authorGitHubUrl: "https://github.com/georgewilliamstrong"
keywords: ["model serving", "serverless", "MLOps", "Instill Cloud"]
---

# Embrace the Flexibility of Serverless Model Serving

Serverless model serving revolutionizes the way you deploy and manage ML models by eliminating the burden of infrastructure management.

In addition to simplifying the development and deployment process, this approach offers scalability, automatically adjusting to demand and handling any number of requests without manual intervention.
It is also highly cost-efficient, as you only pay for the resources you use, eliminating the expense of maintaining costly and underutilized infrastructure.
Finally, it enhances speed, enabling rapid deployment and updates, which significantly accelerates your development and deployment cycles.

## So Why use ‚öóÔ∏è Instill Model?

Choosing **‚öóÔ∏è Instill Model** for serving your machine learning models offers several distinct advantages:
1. **Seamless Integration with Instill VDP**: Instill Model integrates effortlessly with Instill VDP, allowing for a streamlined unstructured data processing and model serving workflow.
2. **Lowest Latency for Data Exchange with Instill VDP**: By utilizing Instill Model, you benefit from minimal latency in data exchange with Instill VDP.
3. **Multi-Region Availability (Coming Soon)**: Instill Model will soon support multi-region deployment across the EU, Asia, and the US, providing global reach and reliability for your models.
4. **AutoML Feature (Coming Soon)**: With the upcoming AutoML feature, Instill Model will enable you to automate the model training and tuning process, making it even easier to optimize your models for deployment.

With **‚òÅÔ∏è Instill Cloud**, which leverages [Kubernetes horizontal auto-scaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/), and the **‚öóÔ∏è Instill Model** MLOps platform harnessing [Ray](https://github.com/ray-project/ray) for seamless model serving, you can focus purely on building and deploying cutting-edge AI solutions without worrying about infrastructure or scaling.

This step-by-step guide will walk you through the process of setting up your own custom model with **‚öóÔ∏è Instill Model** for serverless deployment on **‚òÅÔ∏è Instill Cloud**.

## Prerequisites

To follow this tutorial, and to reap the full benefits of serverless model serving, we recommend that you follow the [quick start guide](/docs/latest/quickstart#authorisation?utm_source=tutorial&utm_medium=link&utm_campaign=model-oss-llm) to set up your **‚òÅÔ∏è Instill Cloud** account.

<CtaButton
  text="‚òÅÔ∏è Try Instill Cloud Free"
  link="https://www.instill.tech/?utm_source=tutorial&utm_medium=link&utm_campaign=model-oss-llm"
/>

‚öóÔ∏è Instill Model uses **Docker** to ensure that machine learning models can be deployed in consistent, isolated, and reproducible environments. Please ensure that you have Docker installed and running by following the [official instructions](https://docs.docker.com/get-docker/).

Please note that this guide assumes you have a basic understanding of machine learning and can code in Python. If you are new to these concepts, we recommend that you look at our [quick start guide](/docs/latest/quickstart#authorisation?utm_source=tutorial&utm_medium=link&utm_campaign=model-oss-llm) which introduces our no-code VDP pipeline builder, and also take a look at some of our other [tutorials](https://www.instill.tech/tutorials).

# Step-by-Step Tutorial

## Step 1: Create a Model Namespace

To get started, log in to your **‚òÅÔ∏è Instill Cloud** account and navigate to the Model page. Click the **Create a Model** button.

This should bring up a configuration window (see image below) where you are able to configure your model settings. For a full description of the available fields, please refer to the [Create a Model](/docs/model/create) documentation.

In this tutorial, we will be walking through how to create and deploy a version of the [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model.
To follow along, please fill in the configuration fields as per the image below, but selecting your own User ID as **Owner**.

<ZoomableImg
src="/tutorial-assets/serverless-model-serving/T1.png"
alt="2. Configure Model Settings"
/>

You have now created an empty model namespace on **‚öóÔ∏è Instill Model**. In the next sections of this tutorial we will show you how to define your own custom model for deployment!

## Step 2: Create a Model Config

To prepare a model to be served on **‚öóÔ∏è Instill Model**, you first need to create your own model directory containing two files - `model.py` and `instill.yaml`.

To configure the [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model, simply open the `instill.yaml` file and populate it with:

<CH.Code>
```yaml instill.yaml
build:
  gpu: true
  python_version: "3.11"
  python_packages:
     - torch==2.2.1
     - transformers==4.36.2
     - accelerate==0.25.0
repo: {user_id}/{model_id}
```
</CH.Code>

Importantly, you must replace `{user_id}` with your **‚òÅÔ∏è Instill Cloud** User ID, and replace `{model_id}` for `tinyllama` - the same Model ID that was specified in [Step 1](#step-1-create-a-model-namespace).

This file specifies the dependencies required to run the model. We will be loading these libraries in the next stage where we define our model class!

## Step 3. Write a Model Script

In this step we will create the `model.py` file, which will contain the model class definition.
This will be broken down in three phases to demonstrate the structure of the model class, and explain the methods it should implement.

### Define the Model Initialization

In the first phase, we import the required packages for model inference and define the `__init__` method to set up the `TinyLlama` model instance.
In this case, we will be using `pipeline()` from the [transformers](https://huggingface.co/docs/transformers/en/index) library to directly load in the [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model.

<CH.Code>
```python model.py
import torch
from transformers import pipeline

from instill.helpers.ray_config import instill_deployment, InstillDeployable


@instill_deployment
class TinyLlama:
    def __init__(self):
        self.pipeline = pipeline(
            "text-generation",
            model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
```
</CH.Code>


The `instill.helpers.ray_config` package contains the decorators and deployment object for the model class, which we will use to convert the model class into a servable model.
These are required to properly define a model class for **‚öóÔ∏è Instill Model**.

### Define the Model Metadata method

In the second phase, we define the `ModelMetadata` method to communicate the models expected input and output shapes to the backend service.
To easily facilitate this, we can make use of the [Python SDK](/docs/latest/sdk/python-sdk) through the `instill.helpers` module which provides a number of functions that can be selected according to the AI task the model performs.

Here, we recognise that the [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model falls under the `text_generation_chat` AI task, and so we will make use of the `construct_text_generation_chat_metadata_response` helper function.

Please refer [here](https://www.instill.tech/docs/model/ai-task) for a full list of the supported AI tasks.

<CH.Code>
```python model.py
# previous imports defined above

from instill.helpers import construct_text_generation_chat_metadata_response


@instill_deployment
class TinyLlama:
    # __init__ defined in phase 1

    def ModelMetadata(self, req):
        return construct_text_generation_chat_metadata_response(req=req)
```
</CH.Code>

### Implement the Inference Method

In the third phase, we implement the inference method `__call__`, which handles the trigger request from **‚öóÔ∏è Instill Model**, contains the necessary logic to run the inference, and constructs the response.
We use the `StandardTaskIO` module to parse the request payload into input parameters, and convert the model outputs to the appropriate response format.

The `TextGenerationChatInput` class from `instill.helpers.const` is used to define the input format for the text generation chat AI task, and the `construct_text_generation_chat_infer_response` function from `instill.helpers` is used to format the model output into the appropriate response format.

<CH.Code>
```python model.py
# previous imports defined above

from instill.helpers.const import TextGenerationChatInput
from instill.helpers.ray_io import StandardTaskIO
from instill.helpers import construct_text_generation_chat_infer_response


@instill_deployment
class TinyLlama:
    # __init__ defined in phase 1

    # ModelMetadata method defined in phase 2

    async def __call__(self, request):
        # parse the request and get the corresponding input for text-generation-chat task
        task_text_generation_chat_input: TextGenerationChatInput = (
            StandardTaskIO.parse_task_text_generation_chat_input(request=request)
        )

        # prepare prompt with chat template
        conv = [
            {
                "role": "system",
                "content": "You are a friendly chatbot",
            },
            {
                "role": "user",
                "content": task_text_generation_chat_input.prompt,
            },
        ]

        prompt = self.pipeline.tokenizer.apply_chat_template(
            conv,
            tokenize=False,
            add_generation_prompt=True,
        )

        # inference
        sequences = self.pipeline(
            prompt,
            max_new_tokens=task_text_generation_chat_input.max_new_tokens,
            do_sample=True,
            temperature=task_text_generation_chat_input.temperature,
            top_k=task_text_generation_chat_input.top_k,
            top_p=0.95,
        )

        # convert the model output into response output
        task_text_generation_chat_output = (
            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)
        )

        return construct_text_generation_chat_infer_response(
            req=request,
            # specify the output dimension
            shape=[1, len(sequences)],
            raw_outputs=[task_text_generation_chat_output],
        )


# now simply declare a global entrypoint for deployment
entrypoint = InstillDeployable(TinyLlama).get_deployment_handle()
```
</CH.Code>

Putting it all together, your `model.py` file should now look like this:

<CH.Code>
```python model.py
import torch
from transformers import pipeline

from instill.helpers.ray_config import instill_deployment, InstillDeployable

from instill.helpers.const import TextGenerationChatInput
from instill.helpers.ray_io import StandardTaskIO
from instill.helpers import (
    construct_text_generation_chat_metadata_response,
    construct_text_generation_chat_infer_response,
)


@instill_deployment
class TinyLlama:
    def __init__(self):
        self.pipeline = pipeline(
            "text-generation",
            model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )

    def ModelMetadata(self, req):
        return construct_text_generation_chat_metadata_response(req=req)

    async def __call__(self, request):
        # parse the request and get the corresponding input for text-generation-chat task
        task_text_generation_chat_input: TextGenerationChatInput = (
            StandardTaskIO.parse_task_text_generation_chat_input(request=request)
        )

        # prepare prompt with chat template
        conv = [
            {
                "role": "system",
                "content": "You are a friendly chatbot",
            },
            {
                "role": "user",
                "content": task_text_generation_chat_input.prompt,
            },
        ]

        prompt = self.pipeline.tokenizer.apply_chat_template(
            conv,
            tokenize=False,
            add_generation_prompt=True,
        )

        # inference
        sequences = self.pipeline(
            prompt,
            max_new_tokens=task_text_generation_chat_input.max_new_tokens,
            do_sample=True,
            temperature=task_text_generation_chat_input.temperature,
            top_k=task_text_generation_chat_input.top_k,
            top_p=0.95,
        )

        # convert the model output into response output
        task_text_generation_chat_output = (
            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)
        )

        return construct_text_generation_chat_infer_response(
            req=request,
            # specify the output dimension
            shape=[1, len(sequences)],
            raw_outputs=[task_text_generation_chat_output],
        )


# now simply declare a global entrypoint for deployment
entrypoint = InstillDeployable(TinyLlama).get_deployment_handle()
```
</CH.Code>

Awesome üòé, you have now defined your own custom model class for model serving with **‚öóÔ∏è Instill Model**. In the next step, we will show you how to build and deploy this model on **‚òÅÔ∏è Instill Cloud**!

## Step 4: Build and Deploy the Model

First, you must ensure that you have the same Python version installed in your local environment as specified in the `instill.yaml` file in [step 2](#step-2-create-a-model-config), in this case `python_version: "3.11"`.

You then need to ensure you have installed the latest version of the [Python SDK](/docs/latest/sdk/python-sdk) by running:

```bash
pip install instill-sdk
```

### Build the Model Image

You can now build your model image by running the following command from within the directory containing the `model.py` and `instill.yaml` files:

```bash
instill-build -t v1 --target-arch amd64
```

This command will build the model image under version tag `v1`. 

Note that we are explicitly specifying the target architecture as `--target-arch amd64`. This is required if you are building on a different architecture (e.g. an ARM machine) to the one you are deploying on (AMD64 in this case).
If this is unspecified, the target architecture will default to that of the system you are building on.

Upon successful completion, you should see a similar output to the following:
```
2024-05-28 01:54:44,404.404 INFO     [Instill Builder] api.instill-inc.tech/{user_id}/tinyllama:v1 built
2024-05-28 01:54:44,423.423 INFO     [Instill Builder] Done
```
where `{user_id}` corresponds to your User ID.

### Push the Model Image

To push model image `v1` to **‚öóÔ∏è Instill Model**, run:

```bash
instill-push -t v1 -u api.instill.tech
```

Upon successful completion, you should see a similar output to the following:
```
2024-05-23 23:05:03,484.484 INFO     [Instill Builder] api.instill-inc.tech/{user_id}/tinyllama:v1 pushed
2024-05-23 23:05:03,485.485 INFO     [Instill Builder] Done
```
where again, the `{user_id}` corresponds to your User ID. 

**‚öóÔ∏è Instill Model** will automatically allocate the resources required by your model and deploy it. Please note that the deployment time varies based on the model size and hardware type and this step could take >1hr.

### Status Check

To check the status of your deployed model version, you can:
1. Navigate back to the **‚òÅÔ∏è Instill Cloud** Models page.
2. Select the `{user_id}/tinyllama` model you created in [Step 1](#step-1-create-a-model-namespace).
3. Click the **Versions** tab, where you will see the corresponding version ID or tag of your pushed model image and the `Status` of deployment.

INSERT IMAGE FROM VERSIONS TAB.

The `Status` will initially show as `Starting`, indicating that your model is offline and **‚öóÔ∏è Instill Model** is still in the process of allocating resources and deploying it.
Once this status changes to `Active`, the model resources have been provisioned, and your model is ready to serve requests.

## Step 5. Inference

Once your model is deployed and active, you can easily test its behaviour following these steps:
1. Navigating to the **Overview** tab for your `{user_id}/tinyllama` model.
2. Enter a prompt in the **Input** pane.
3. Scroll down and hit **Run** to trigger the model inference.

INSERT IMAGE WITH 1 AND 2 FROM OVERVIEW TAB.

You should see a response generated by the [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model in the **Output** pane.

To access model inferences via the API:
1. Navigate to the **API** tab for your `{user_id}/tinyllama` model.
2. Follow the instructions by setting your own `INSTILL_API_TOKEN` as an environment variable:
   ```bash
   $ export INSTILL_API_TOKEN=********
   ```
   and using the provided `curl` command to send a request to the model endpoint:
   ```bash
   curl --location 'https://api.instill.tech/model/v1alpha/users/george_strong/models/tinyllama/trigger' \
   --header 'Content-Type: application/json' \
   --header 'Authorization: Bearer $INSTILL_API_TOKEN' \
   --data '{
     "task_inputs": [
       {
         "text_generation": {
           "prompt": "How is the weather today?",
            "chat_history": [
              {
                "role": "user",
                "content": [
                  {
                    "type": "text",
                    "text": "hi"
                  }
                ]
              }
            ],
            "system_message": "you are a helpful assistant",
            "max_new_tokens": 1024,
            "top_k": 5,
            "temperature": 0.7
          }
        }
      ]
   }'
   ```

# Conclusion

Congratulations üéâ! You have successfully harnessed **‚òÅÔ∏è Instill Cloud** and **‚öóÔ∏è Instill Model** to create, deploy, and serve a custom machine learning model in a serverless environment.
This setup allows you to focus on innovation without worrying about infrastructure and scaling, streamlining your MLOps workflows with flexibility and efficiency.

### Recap
In this tutorial, you have learned how to:

1. **Create a Model Namespace**: Initialize a new model namespace on **Instill Model**.
2. **Define a Model Configuration**: Set up the `instill.yaml` config file for your model.
3. **Write a Model Script**: Implement the model class containing the model initialization, metadata, and inference methods.
4. **Build and Deploy the Model**: Build the model image and deploy it to **‚öóÔ∏è Instill Model** hosted on **‚òÅÔ∏è Instill Cloud**.
5. **Run Inference**: Test your deployed model and interact with it via the **‚òÅÔ∏è Instill Model** UI or API.

By leveraging these features, **‚öóÔ∏è Instill Model** stands out as a powerful choice for your serverless model serving requirements.

Ready to deploy and scale your own models?
<CtaButton
text="‚òÅÔ∏è Try Instill Cloud Free"
link="https://www.instill.tech/?utm_source=tutorial&utm_medium=link&utm_campaign=model-oss-llm"
/>

Thank you for following this tutorial! We hope it has been helpful in guiding you through the process of serverless model serving with **‚öóÔ∏è Instill Model**. Stay tuned for more tutorials and updates! üöÄ
