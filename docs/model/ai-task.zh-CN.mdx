---
title: "AI Task"
lang: "zh-CN"
draft: false
description: "Learn about Instill Model, the MLOps/LLMOps platform, in the project Instill Core: https://github.com/instill-ai/instill-core"
---

When you create a model in **‚öóÔ∏è Instill Model**, it's necessary to define the
standardized AI task that the model falls under.

In a data pipeline, a model serves as a critical component, designed to tackle a
specific AI task. By standardizing the data format of model outputs into AI
tasks, models become modular: you can interchange different model sources as an
**AI** component in a **üíß Instill VDP** pipeline as long as they're designed
for the same AI task. **‚öóÔ∏è Instill Model** also adheres to the standard format
of AI tasks for data integration in the **üíß Instill VDP** pipelines.

Currently, **‚öóÔ∏è Instill Model** outlines the data interface for popular tasks:

- `Image Classification`: Categorizing images into predefined classes
- `Object Detection`: Identifying and localizing multiple objects in images
- `Keypoint Detection`: Identifying and localizing multiple keypoints of
  objects in images
- `OCR (Optical Character Recognition)`: Identifying and recognizing text in
  images
- `Instance Segmentation`: Identifying, localizing, and outlining multiple
  objects in images
- `Semantic Segmentation`: Categorizing image pixels into predefined classes
- `Text to Image`: Generating images from input text prompts
- `Image to Image`: Generating images from input image prompts
- `Text Generation`: Generating texts from input text prompts
- `Text Generation Chat`: Generating chat style texts from input text prompts
- `Visual Question Answering`: Generating chat style texts from input text
  and image prompts
- The list is expanding ... üå±

The tasks listed above concentrate on analyzing and understanding the content of
unstructured data in a manner similar to human cognition. The objective is to
enable a computer/device to provide a description for the data that is as
comprehensive and accurate as possible. These primitive tasks form the basis for
building numerous real-world industrial AI applications. Each task is elaborated
in the respective section below.

## Image Classification

Image Classification is a Vision task to assign a single pre-defined category
label to an entire input image. Generally, an Image Classification model takes
an image as the input, and outputs a prediction about what category this image
belongs to and a confidence score (usually between 0 and 1) representing the
likelihood that the prediction is correct.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-classification.svg"
  alt="Image Classification task"
/>

```json
{
  "task": "TASK_CLASSIFICATION",
  "task_outputs": [
    {
      "classification": {
        "category": "golden retriever",
        "score": 0.98
      }
    }
  ]
}
```

**Available models**

| Model                                                                                    | Sources                                                                                                                      | Framework | CPU | GPU |
| ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | --------- | --- | --- |
| [MobileNet v2](https://github.com/onnx/models/tree/main/vision/classification/mobilenet) | [GitHub](https://github.com/instill-ai/model-mobilenetv2), [GitHub-DVC](https://github.com/instill-ai/model-mobilenetv2-dvc) | ONNX      | ‚úÖ  | ‚úÖ  |
| [Vision Transformer (ViT)](https://huggingface.co/google/vit-base-patch16-224)           | [Hugging Face](https://huggingface.co/google/vit-base-patch16-224)                                                           | ONNX      | ‚úÖ  | ‚ùå  |

## Object Detection

Object Detection is a Vision task to localise multiple objects of pre-defined
categories in an input image. Generally, an Object Detection model receives an
image as the input, and outputs bounding boxes with category labels and
confidence scores on detected objects.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-detection.svg"
  alt="Object
  Detection task"
/>

```json
{
  "task": "TASK_DETECTION",
  "task_outputs": [
    {
      "detection": {
        "objects": [
          {
            "category": "dog",
            "score": 0.97,
            "bounding_box": {
              "top": 102,
              "left": 324,
              "width": 208,
              "height": 405
            }
          },
          ...
        ]
      }
    }
  ]
}
```

**Available models**

| Model                                          | Sources                                                      | Framework | CPU | GPU |
| ---------------------------------------------- | ------------------------------------------------------------ | --------- | --- | --- |
| [YOLOv4](https://github.com/AlexeyAB/darknet)  | [GitHub-DVC](https://github.com/instill-ai/model-yolov4-dvc) | ONNX      | ‚úÖ  | ‚úÖ  |
| [YOLOv7](https://github.com/WongKinYiu/yolov7) | [GitHub-DVC](https://github.com/instill-ai/model-yolov7-dvc) | ONNX      | ‚úÖ  | ‚úÖ  |

## Keypoint Detection

Keypoint Detection task a Vision task to localise multiple objects by
identifying their pre-defined keypoints, for example, identifying the keypoints
of human body: nose, eyes, ears, shoulders, elbows, wrists, hips, knees and
ankles. Normally, a Keypoint Detection task takes an image as the input, and
outputs the coordinates and visibility of keypoints with bounding boxes and
confidence scores on detected objects.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-keypoint.svg"
  alt="Keypoint
  Detection task"
/>

```json
{
  "task": "TASK_KEYPOINT",
  "task_outputs": [
    {
      "keypoint": {
        "objects": [
          {
            "keypoints": [
              {
                "v": 0.53722847,
                "x": 542.82764,
                "y": 86.63817
              },
              {
                "v": 0.634061,
                "x": 553.0073,
                "y": 79.440636
              },
              ...
            ],
            "score": 0.94,
            "bounding_box": {
              "top": 86,
              "left": 185,
              "width": 571,
              "height": 203
            }
          },
          ...
        ]
      }
    }
  ]
}
```

**Available models**

| Model                                                  | Sources                                                           | Framework | CPU | GPU |
| ------------------------------------------------------ | ----------------------------------------------------------------- | --------- | --- | --- |
| [YOLOv7 W6 Pose](https://github.com/WongKinYiu/yolov7) | [GitHub-DVC](https://github.com/instill-ai/model-yolov7-pose-dvc) | ONNX      | ‚úÖ  | ‚úÖ  |

## Optical Character Recognition (OCR)

OCR is a Vision task to localise and recognise text in an input image. The task
can be done in two steps by multiple models: a text detection model to detect
bounding boxes containing text and a text recognition model to process typed or
handwritten text within each bounding box into machine readable text.
Alternatively, there are deep learning models that can accomplish the task in
one single step.

<ZoomableImg src="/docs-assets/core-concepts/ai-task-ocr.svg" alt="OCR task" />

```json
{
  "task": "TASK_OCR",
  "task_outputs": [
    {
      "ocr": {
        "objects": [
          {
            "text": "ENDS",
            "score": 0.99,
            "bounding_box": {
              "top": 298,
              "left": 279,
              "width": 134,
              "height": 59
            }
          },
          {
            "text": "PAVEMENT",
            "score": 0.99,
            "bounding_box": {
              "top": 228,
              "left": 216,
              "width": 255,
              "height": 65
            }
          }
        ]
      }
    }
  ]
}
```

**Available models**

| Model                                                                                                                          | Sources                                                   | Framework | CPU | GPU |
| ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------- | --------- | --- | --- |
| [PSNet](https://github.com/open-mmlab/mmocr/tree/main/configs/textdet/psenet) + [EasyOCR](https://github.com/JaidedAI/EasyOCR) | [GitHub-DVC](https://github.com/instill-ai/model-ocr-dvc) | ONNX      | ‚úÖ  | ‚úÖ  |

## Instance Segmentation

Instance Segmentation is a Vision task to detect and delineate multiple objects
of pre-defined categories in an input image. Normally, the task takes an image
as the input, and outputs uncompressed run-length encoding (RLE) representations
(a variable-length comma-delimited string), with bounding boxes, category labels
and confidence scores on detected objects.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-instance-segmentation.svg"
  alt="Instance Segmentation task"
/>

Run-length encoding (RLE) is an efficient form to store binary masks. It is
commonly used to encode the location of foreground objects in segmentation. We
adopt [the uncompressed RLE definition used in the COCO
dataset](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/mask.py).
It divides a binary mask (must in column-major order) into a series of piecewise
constant regions and for each piece simply stores the length of that piece.

<div align="center">
  <ZoomableImg
    src="/docs-assets/core-concepts/ai-task-mask-rle.svg"
    alt="Examples of encoding masks into RLEs and decoding masks encoded via RLEs"
    width="320px"
  />
</div>

The above image shows examples of encoding masks into RLEs and decoding masks
encoded via RLEs. Note that the odd counts in the RLEs are always the numbers of
zeros.

<InfoBlock type="info" title="info">
  {" "}
  Check out functions to [encode masks into RLEs](https://github.com/instill-ai/instill-core/blob/e737d0b1a6d8618b44bee24f502778d38abecc92/examples/streamlit/instance_segmentation/utils.py#L18-L39)
  and [decode masks encoded via RLEs](https://github.com/instill-ai/instill-core/blob/e737d0b1a6d8618b44bee24f502778d38abecc92/examples/streamlit/instance_segmentation/utils.py#L41-L59).
</InfoBlock>

```json
{
  "task": "TASK_INSTANCE_SEGMENTATION",
  "task_outputs": [
    {
      "instance_segmentation": {
        "objects": [
          {
            "rle": "2918,12,382,33,...",
            "score": 0.99,
            "bounding_box": {
              "top": 95,
              "left": 320,
              "width": 215,
              "height": 406
            },
            "category": "dog"
          },
          {
            "rle": "34,18,230,18,...",
            "score": 0.97,
            "bounding_box": {
              "top": 194,
              "left": 130,
              "width": 197,
              "height": 248
            },
            "category": "dog"
          }
        ]
      }
    }
  ]
}
```

**Available models**

| Model                                                                                                                       | Sources                                                                     | Framework | CPU | GPU |
| --------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | --------- | --- | --- |
| [Mask RCNN](https://github.com/onnx/models/blob/main/vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-10.onnx) | [GitHub-DVC](https://github.com/instill-ai/model-instance-segmentation-dvc) | PyTorch   | ‚úÖ  | ‚úÖ  |

## Semantic Segmentation

Semantic Segmentation is a Vision task of assigning a class label to every pixel
in the image. Normally, the task takes an image as the input, and outputs
segmentation mask (RLE) representations (a variable-length comma-delimited
string) for each group of pixel objects and category of the group objects.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-semantic-segmentation.svg"
  alt="Semantic Segmentation task"
/>

```json
{
  "task": "TASK_SEMANTIC_SEGMENTATION",
  "task_outputs": [
    {
      "semantic_segmentation": {
        "stuffs": [
          {
            "rle": "2918,12,382,33,...",
            "category": "person"
          },
          {
            "rle": "34,18,230,18,...",
            "category": "sky"
          },
          ...
        ]
      }
    }
  ]
}
```

**Available models**

| Model                                                                                                                                               | Sources                                                                     | Framework | CPU | GPU |
| --------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | --------- | --- | --- |
| [Lite R-ASPP based on MobileNetV3](https://github.com/open-mmlab/mmsegmentation/tree/98dfa1749bac0b5281502f4bb3832379da8feb8c/configs/mobilenet_v3) | [GitHub-DVC](https://github.com/instill-ai/model-semantic-segmentation-dvc) | ONNX      | ‚úÖ  | ‚úÖ  |

## Text to Image

Text to Image is a Generative AI task to generate images from text inputs.
Generally, the task takes descriptive text prompts as the input, and outputs
generated images in Base64 format based on the text prompts.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-text-to-image.svg"
  alt="Text to Image task"
/>

```json
{
  "task": "TASK_TEXT_TO_IMAGE",
  "task_outputs": [
    {
      "text_to_image": {
        "images": ["/9j/4AAQSkZJRgABAQAAAQABAAD/..."]
      }
    }
  ]
}
```

<InfoBlock type="info" title="decode base64 images">

In above example, the generated images is a list of Base64 encoded images. To
obtain the images, we need to decode Base64 as below snippet code.

```python
import base64
import numpy as np

# Decode the first image result

base64_image = out['text_to_image']['images'][0]
image = base64.b64decode(base64_image)

# Save the decoded image

filename = 'text_to_image.jpg'
with open(filename, 'wb') as f:
f.write(image)

```

</InfoBlock>

**Available models**

| Model                                                                     | Sources                                                                                                                                                                                                                                                      | Framework | CPU | GPU |
| ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------- | --- | --- |
| [Stable Diffusion](https://huggingface.co/runwayml/stable-diffusion-v1-5) | [GitHub-DVC](https://github.com/instill-ai/model-diffusion-dvc), [Local-CPU](https://artifacts.instill.tech/vdp/sample-models/stable-diffusion-1-5-cpu.zip), [Local-GPU](https://artifacts.instill.tech/vdp/sample-models/stable-diffusion-1-5-fp16-gpu.zip) | ONNX      | ‚úÖ  | ‚úÖ  |
| [Stable Diffusion XL](https://huggingface.co/papers/2307.01952)           | [GitHub-DVC](https://github.com/instill-ai/model-diffusion-xl-dvc)                                                                                                                                                                                           | PyTorch   | ‚ùå  | ‚úÖ  |

<InfoBlock type="tip" title="tip">

Importing [Stable
Diffusion](https://github.com/instill-ai/model-diffusion-dvc) from GitHub will
take a while. Alternatively, you can download the model locally as a one-time
effort.

**Step 1**: Download **Stable Diffusion v1.5 CPU** sample model.

```bash
curl https://artifacts.instill.tech/vdp/sample-models/stable-diffusion-1-5-cpu.zip --output stable-diffusion-1-5-cpu.zip
```

**Step2**: Refer to the guideline on **importing local models via
[no-code](./import/local#no-code-setup) or
[low-code](./import/local#low-code-setup)**.

</InfoBlock>

## Text Generation

Text Generation is a Generative AI task to generate new text from text inputs.
Generally, the task takes incomplete text prompts as the input, and produces new
text based on the prompts. The task can fill in incomplete sentences or even
generate full stories given the first words.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-text-generation.svg"
  alt="Text Generation task"
/>

```json
{
  "task": "TASK_TEXT_GENERATION",
  "task_outputs": [
    {
      "text_generation": {
        "text": "The winds of change are blowing strong, bring new beginnings, righting wrongs. The world around us is constantly turning, and with each sunrise, our spirits are yearning."
      }
    }
  ]
}
```

**Available models**

| Model                                                       | Sources                                                                  | Framework   | CPU | GPU |
| ----------------------------------------------------------- | ------------------------------------------------------------------------ | ----------- | --- | --- |
| [Llama2](https://huggingface.co/meta-llama/Llama-2-7b)      | [GitHub-DVC](https://github.com/instill-ai/model-llama2-7b-dvc)          | Transformer | ‚ùå  | ‚úÖ  |
| [Code Llama](https://github.com/facebookresearch/codellama) | [GitHub-DVC](https://github.com/instill-ai/model-codellama-7b-dvc)       | Transformer | ‚ùå  | ‚úÖ  |
| [Llama3-instruct](https://llama.meta.com/llama3)            | [GitHub-DVC](https://github.com/instill-ai/model-llama3-8b-instruct-dvc) | Transformer | ‚ùå  | ‚úÖ  |

<InfoBlock type="tip" title="tip">

Depending on your internet speed, importing
LLM models will take a while.

Some models only supports GPU deployment. By default, **‚öóÔ∏è Instill Model** can access all your
GPUs.

</InfoBlock>

## Text Generation Chat

Text Generation Chat is a Generative AI task to generate new text from text
inputs in chat style. Generally, the task takes a series of conversation as the
input, and produces new response on the prompts. The task can perform
conversation and even answer question based on previous context.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-text-generation.svg"
  alt="Text Generation Chat task"
/>

```json
{
  "task": "TASK_TEXT_GENERATION_CHAT",
  "task_outputs": [
    {
      "text_generation_chat": {
        "text": "What a delicate situation!\n\nI must advise that it's generally not a good idea to..."
      }
    }
  ]
}
```

**Available models**

| Model                                                               | Sources                                                              | Framework   | CPU | GPU |
| ------------------------------------------------------------------- | -------------------------------------------------------------------- | ----------- | --- | --- |
| [Llama2 Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) | [GitHub-DVC](https://github.com/instill-ai/model-llama2-7b-chat-dvc) | Transformer | ‚ùå  | ‚úÖ  |
| [MosaicML MPT](https://huggingface.co/mosaicml/mpt-7b)              | [GitHub-DVC](https://github.com/instill-ai/model-mpt-7b-dvc)         | Transformer | ‚ùå  | ‚úÖ  |
| [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)         | [GitHub-DVC](https://github.com/instill-ai/model-mistral-7b-dvc)     | Transformer | ‚ùå  | ‚úÖ  |
| [Zephyr-7b](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)   | [GitHub-DVC](https://github.com/instill-ai/model-zephyr-7b-dvc)      | Transformer | ‚úÖ  | ‚úÖ  |

<InfoBlock type="tip" title="tip">

Depending on your internet speed, importing
LLM models will take a while.

Some models only supports GPU deployment. By default, **‚öóÔ∏è Instill Model** can access all your
GPUs.

</InfoBlock>

## Visual Question Answering

Visual Question Answering (VQA) is a task in computer vision that involves
answering questions about an image. The goal of VQA is to teach machines to
understand the content of an image and answer questions about it in natural
language.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-text-generation.svg"
  alt="Visual Question Answering"
/>

```json
{
  "task": "TASK_VISUAL_QUESTION_ANSWERING",
  "task_outputs": [
    {
      "visual_question_answering": {
        "text": "The image appears to show a close-up view of a plant's leaf or a similar plant part."
      }
    }
  ]
}
```

**Available models**

| Model                                                                | Sources                                                         | Framework   | CPU | GPU |
| -------------------------------------------------------------------- | --------------------------------------------------------------- | ----------- | --- | --- |
| [Llava-1-6](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-13b) | [GitHub-DVC](https://github.com/instill-ai/model-llava-13b-dvc) | Transformer | ‚ùå  | ‚úÖ  |

<InfoBlock type="tip" title="tip">

Depending on your internet speed, importing
LLM models will take a while.

Some models support only GPU deployment. By default, **‚öóÔ∏è Instill Model** can access all your
GPUs.

</InfoBlock>

## Unspecified Task

**‚öóÔ∏è Instill Model** is very flexible and allows models even if the task
is not standardised yet or the output of the model can't be converted to the
format of supported AI tasks. The model will be classified as an `Unspecified`
task.

<ZoomableImg
  src="/docs-assets/core-concepts/ai-task-unspecified.svg"
  alt="Unspecified task"
/>

```json
{
  "unspecified": {
    "raw_outputs": [
      {
        "data": [0.85, 0.1, 0.05],
        "data_type": "FP32",
        "name": "output_scores",
        "shape": [3]
      },
      {
        "data": ["dog", "cat", "rabbit"],
        "data_type": "BYTES",
        "name": "output_labels",
        "shape": [3]
      }
    ]
  }
}
```

## Suggest a New Task

Currently, the model output is converted to standard format based on the AI task
outputs maintained in
[Protobuf](https://github.com/instill-ai/protobufs/tree/main/vdp/model/v1alpha).

If you'd like to have **support for a new task**, you can request it in the
**#give-feedback** channel on [Discord](https://discord.gg/sevxWsqpGh).
