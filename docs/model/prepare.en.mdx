---
title: "Prepare a Model"
lang: "en-US"
draft: false
description: "Learn about how to prepare your models for the MLOps tool Instill Model https://github.com/instill-ai/model"
---

To prepare a model version to be served on **Instill Model**, you follow below steps locally on your PC:

- Create a [model config](#model-config) `instill.yaml` to describe your model's dependencies
- Write a `model.py` file that defined the model class which will be decorated into a servable model with Instill's `python-sdk`
- Organize the model files into valid model [layout](#model-layout)

## Model Config

Model config is a `instill.yaml` file that accompanies the model to describe dependency information. It is crucial for reproducibility, sharing and discoverability.

In the model config, you can provide information about:

- `build`
  - `gpu`: `required` `boolean` If the model needs GPU
  - `python_version`: `required` `string` supports `3.9`, `3.10` and `3.11`
  - `cuda_version`: `string` suuports `11.5`, `11.6`, `11.7`, `11.8`, `12.1`. If not presented or empty, default to the latest version.
  - `system_packages`: `list` model image is based on ubuntu 20.04 LTS, packages listed here will be installed from apt package manager
  - `python_packages`: `required` `list` packages listed here will be installed with `pip`
- `repo`: `required` `string` your Instill Core namespace with the format `{user_id}/{model_id}`

Following is an example config file for a `TinyLlama` model which is intended to be run on GPU

<CH.Code>
```yaml instill.yaml
build:
  gpu: true
  python_version: "3.11"  # support only 3.11
  python_packages:
    - torch==2.2.1
    - transformers==4.36.2
    - accelerate==0.25.0
# the format for repo is {user_id}/{model_id}
# make sure the model id matched what you created on console
repo: instill-ai/tinyllama
```
</CH.Code>

## Model Layout

With [Ray](https://github.com/ray-project/ray) under the hood for model serving, Instill Model extends its support to any arbitrary deep learning frameworks you desires.
To deploy a model on Instill Model, we suggest you prepare the model files similar to the following layout:

```shellscript
.
├── instill.yaml
├── model.py
├── <additional_modules>
└── <weights>
    ├── <weight_file_1>
    ├── <weight_file_2>
    ├── ...
    └── <weight_file_n>
```

The above layout displays a typical Instill Model model consisting of

- `instill.yaml` - [model config](#model-config) file that describe the dependencies
- `model.py` - this is where you defined the decorated model class that contains custom inference logic
- `<additional_modules>` - a directory that holds the python modules files if necessary
- `<weights>` - a directory that holds the weight files if necessary

You can name the `<weights>` and `<additional_modules>` folder freely provided that the folder name are clear and semantic.

## Prepare Model Code

To implement a custom model that can be imported and served on `Instill Model`, you only need to implement a simple model class within the `model.py` file

The custom model class will need to implement the following methods
- `__init__`
  - within the `__init__` function, this is where to define the model loading process, allowing the weights to be store in memory and allow faster auto-scaling behavior
- `ModelMetadata`
  - `ModelMetadata` method tells the backend service what is the expected input/output shape the model is expecting, if you are using our predefined [AI Tasks](/docs/model/ai-task.en.mdx), you can simply import construct_\{task\}_metadata_reponse and use it as return
- `__call__`
  - `__call__` is the inference request entrypoint, this is where you implement your model inference logic.

Following is a simple implementation of [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model with explanations.

```python
# import neccessary packages
import torch
from transformers import pipeline

# import SDK helper functions
# const package hosts the standard Datatypes and Input class for each standard Instill AI Tasks
from instill.helpers.const import TextGenerationChatInput

# ray_io package hosts the parsers to easily convert request payload into input paramaters, and model outputs to response
from instill.helpers.ray_io import StandardTaskIO

# ray_config package hosts the decorators and deployment object for model class
from instill.helpers.ray_config import instill_deployment, InstillDeployable
from instill.helpers import (
    construct_text_generation_chat_infer_response,
    construct_text_generation_chat_metadata_response,
)


# use instill_deployment decorator to convert the model class to servable model
@instill_deployment
class TinyLlama:
    # within the __init__ function, setup the model instance with the desired framework, in this
    # case is the pipeline from transformers
    def __init__(self):
        self.pipeline = pipeline(
            "text-generation",
            model="tinyllama",
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )

    # ModelMetadata tells the server what input and output shapes the model is expecting
    def ModelMetadata(self, req):
        return construct_text_generation_chat_metadata_response(req=req)

    # ModelInfer is the method handling the trigger request from Instill Model
    async def __call__(self, request):
        # use StandardTaskIO package to parse the request and get the corresponding input
        # for text-generation-chat task
        task_text_generation_chat_input: TextGenerationChatInput = (
            StandardTaskIO.parse_task_text_generation_chat_input(request=request)
        )

        # prepare prompt with chat template
        prompt = self.pipeline.tokenizer.apply_chat_template(
            task_text_generation_chat_input.chat_history,
            tokenize=False,
            add_generation_prompt=True,
        )

        # inference
        sequences = self.pipeline(
            prompt,
            max_new_tokens=task_text_generation_chat_input.max_new_tokens,
            do_sample=True,
            temperature=task_text_generation_chat_input.temperature,
            top_k=task_text_generation_chat_input.top_k,
            top_p=0.95,
        )

        # convert the output into response output with again the StandardTaskIO
        task_text_generation_chat_output = (
            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)
        )

        return construct_text_generation_chat_infer_response(
            req=request,
            # specify the output dimension
            shape=[1, len(sequences)],
            raw_outputs=[task_text_generation_chat_output],
        )


# now simply declare a global entrypoint for deployment
entrypoint = InstillDeployable(TinyLlama).get_deployment_handle()
```

After all the required model files are prepared, you can now [build](build) the custom model image with verion as tag.
