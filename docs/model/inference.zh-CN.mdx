---
title: "Model Inference"
lang: "zh-CN"
draft: false
description: "Learn about Instill Model, the MLOps/LLMOps platform, in the project Instill Core: https://github.com/instill-ai/instill-core"
---

**‚öóÔ∏è Instill Model** provides an automated model inference server. You can
perform an inference either from the **Model Overview** page, or via an **API
Endpoint**.

<InfoBlock type="info" title="info">  
  In the context of Machine Learning (ML) and Artificial Intelligence (AI), the
  term inference is often compared with training. An **inference** is where
  capabilities learnt during model training are used to "infer" a result,
  typically a prediction about the input data.
</InfoBlock>

## Inference in the Model Overview Page

Navigate to and select your chosen model to bring up its corresponding **Model
Overview** page.

1. Provide the necessary data or upload files as inputs.
2. Click the `Run` button to perform an inference from the model and receive the
   results in the output window.

<InfoBlock type="tip" title="tip">  
  To ensure that the model version you are running is able to serve inference
  requests, you can check the status of the model version by selecting the
  **Versions** tab. To learn more about this please refer to the [Model
  State](model-state) page.
</InfoBlock>

## Inference via API Endpoint

Once a model version is deployed, it automatically creates a unique **API
Endpoint** for model inference at
`/users/USER_ID/models/MODEL_ID/versions/VERSION_TAG/trigger`.

This endpoint allows you to send multiple images in popular formats (.PNG and
.JPEG) in a single request. As shown in the examples below, the API is designed
to accept batched images:
1. Sent by remote URL,
2. Sent as Base64 encoded strings,
3. Uploaded by multipart.

<CH.Code>

```shellscript cURL(url)
curl -X POST http://localhost:8080/v1alpha/users/USER_ID/models/MODEL_ID/versions/VERSION_TAG/trigger \
--header 'Authorization: Bearer instill_sk_***' \
--data '{
    "task_inputs": [
        {
            "classification": {
                "image_url": "https://artifacts.instill.tech/imgs/dog.jpg"
            }
        },
        {
            "classification": {
                "image_url": "https://artifacts.instill.tech/imgs/bear.jpg"
            }
        }
    ]
}'
```

```shellscript cURL(base64)
curl -X POST http://localhost:8080/v1alpha/users/USER_ID/models/MODEL_ID/versions/VERSION_TAG/trigger \
--data '{
    "task_inputs": [
        {
            "classification": {
                "image_base64": "/9j/4AAQSkZ...iiigD/2Q=="
            }
        },
        {
            "classification": {
                "image_base64": "/9j/4AAQSkZ...iiigD/2Q=="
            }
        }
    ]
}'
```

```shellscript cURL(multipart)
curl -X POST http://localhost:8080/v1alpha/users/USER_ID/models/MODEL_ID/versions/VERSION_TAG/trigger-multipart \
--header 'Authorization: Bearer instill_sk_***' \
--form 'file=@"dog.jpg"' \
--form 'file=@"bear.jpg"'
```

</CH.Code>

The `USER_ID`, `MODEL_ID` and `VERSION_TAG` correspond to the user namespace,
the ID of the model and the version tag of the model.

## Connect Models in **üíß Instill VDP**

To build pipelines for your AI workflows with models served in **‚öóÔ∏è Instill
Model** , you can utilize the [AI component](../component/ai/instill) for **‚öóÔ∏è
Instill Model** within **üíß Instill VDP**.
