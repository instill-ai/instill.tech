---
title: "Model Inference"
lang: "en-US"
draft: false
description: "Learn about how to perform model inference into the MLOps tool Instill Model https://github.com/instill-ai/model"
---

Upon successful model import, the state of a model can be `UNSPECIFIED`, `OFFLINE`, `ONLINE` or `ERROR`.

- When a model is initially created, the states is by default `OFFLINE`.
- A model can be switched to `OFFLINE` state by invoking the `model-backend` endpoint `/undeploy` only when its original state is `ONLINE`.
- A model can be switch to `ONLINE` state by invoking the `model-backend` endpoint `/deploy` only when its original state is `OFFLINE`. Model deployment operation can take time depending on factors like Internet connection and model size. Before a model is deployed online, the state will be `UNSPECIFIED`.
- If the state of a model ends up with `ERROR`, it is undeployable on Instill Model. Please refer to [Prepare Models](./prepare-models) to make your model Instill Model ready.

<div align="center">
  <ZoomableImg
    src="/docs-assets/core-concepts/model-state.svg"
    alt="  The finite-state-machine (FSM) diagram for the model state transition logic"
    width="600px"
  />
</div>

## Model Inference

An **inference** is a prediction to a question or task. In the concept of Machine Learning (ML) and Artificial Intelligence (AI), the term inference is often compared with training.
To put it simple, inference is where capabilities learnt during training are put to analyze data to "infer" a result.
Inference can be found and are applied everywhere across industries from photo tagging to autonomous driving.

_Instill Model provides an automated model inference server._
After importing a model from a supported source, such as GitHub or Hugging Face, and deploying it online,
it dynamically generates dedicated API endpoints for model inference.
You can build VDP pipelines for your AI workflows using the [Instill Model AI Connector](../vdp/connectors/instill-model).

After deploying a model, you can send multiple images of popular formats (PNG and JPEG) in one request to the generated model API endpoint.
Check the examples below. The API accepts batched images

- sent by remote URL and Base64 or
- uploaded by multipart.

<CH.Code>

```shellscript cURL(url)
curl -X POST http://localhost:9080/v1alpha/users/{user-id}/models/{id}/test -d '{
  "inputs": [
    {
      "image_url": "https://artifacts.instill.tech/imgs/dog.jpg"
    },
    {
      "image_url": "https://artifacts.instill.tech/imgs/horse.jpg"
    }
  ]
}'
```

```shellscript cURL(base64)
curl -X POST http://localhost:9080/v1alpha/users/{user-id}/models/{id}/test -d '{
  "inputs": [
    {
      "image_base64": "/9j/4AAQSkZ...iiigD/2Q=="
    },
    {
      "image_base64": "/9j/4QBLRXh...UR5+f/2Q=="
    }
  ]
}'
```

```shellscript cURL(multipart)
curl -X POST http://localhost:9080/v1alpha/users/{user-id}/models/{id}/test-multipart \
--form 'file=@"dog.jpg"' \
--form 'file=@"horse.jpg"'
```

</CH.Code>

in which `{user-id}` and `{id}` corresponds to the namespace and ID of a model.
