import { DocsLayout } from "@/components/docs";
export const meta = {
  title: "Model",
  lang: "en-US",
  draft: false,
  description:
    "One-click import and deploy ML models with the open-source visual data ETL tool VDP https://github.com/instill-ai/vdp",
  date: "",
};

export default ({ children }) => (
  <DocsLayout meta={meta}>{children}</DocsLayout>
);

A **Model** component is an algorithm run on unstructured visual data to solve a certain [CV Task](/docs/core-concepts/core-concepts/cv-task).

VDP uses Triton Inference server for model serving. It supports multiple deep learning frameworks including [TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org/), [TensorRT](https://developer.nvidia.com/tensorrt) and [ONNX](https://onnx.ai).
Besides, the [Python Backend](https://github.com/triton-inference-server/python_backend) enables Triton to support any model written in Python.
To make your models VDP-ready, please refer to [Prepare models](/docs/prepare-models/overview).

## Definition

VDP uses [ModelDefinition](https://github.com/instill-ai/protobufs/blob/main/vdp/model/v1alpha/model_definition.proto#L30-L76) to define how to configure and import a model from a supported model source.
Please check out [Import Models](/docs/import-models/overview) to learn more.

Instill AI develops and maintains model sources (`ModelDefinition`). We use [release stage](overview#release-stage) to indicate a model source's readiness.

## Model Instance

A **Model Instance** represents a tagged snapshot of a Model. A model may have multiple model instances.
The tag of a model instance depends on the model source and what versioning control tool the model source uses.

_VDP captures the versioning of your models via model instances._
During model development, ML teams version models to track and manage changes and collaborate between members.
As you use VDP to import your model, different versions of your model are also imported as tagged model instances.
In this case, you can keep a single history of your models and the original versioning serves as the single source of truth.

Throughout the documentation, we use the two terms _model_ and _model instance_ interchangeably.
For example, we say that model is the core component of data pipeline to convert the unstructured visual data to structured insights.
This is a high level concept, but underneath model instances are used to construct a data pipeline.
A VDP pipeline can have multiple model instances. The examples below showcase pipeline recipes that incorporate single or multiple model instances.

<CH.Code>

```json single-model-instance-in-pipeline
{
  "source": "source-connectors/source-http",
  "model_instances": ["models/model-1/instances/v1.0"],
  "destination": "destination-connectors/postgres-db"
}
```

```json multiple-model-instances-in-pipeline
{
  "source": "source-connectors/source-http",
  "model_instances": [
    "models/model-1/instances/v1.0",
    "models/model-2/instances/v0.3"
  ],
  "destination": "destination-connectors/source-destination"
}
```

</CH.Code>

## Model deployment

When importing a model in VDP, all versions of the model are imported as model instances associated with the original version tags.
Please refer to [Import Models](/docs/import-models/overview) to learn about model versioning with supported model sources.
Among all model instances, you can choose specific ones into the model repository to deploy and then to use in data pipelines.

![VDP model and model instance](/docs-assets/model-and-model-instance.svg)

## State

The state of a model instance can be `UNSPECIFIED`, `OFFLINE`, `ONLINE` or `ERROR`.

When a model is initially created, the states of all model instances of this model are by default `OFFLINE`.

A model instance can be switched to `OFFLINE` state by invoking the `model-backend` endpoint `:undeploy` only when its original state is `ONLINE`.

A model instance can be switch to `ONLINE` state by invoking the `model-backend` endpoint `:deploy` only when its original state is `OFFLINE`.
Model deployment operation can take time depending on factors like Internet connection and model size.
Before a model instance is deployed online, the state of the model instance will be `UNSPECIFIED`.

If the state of a model instance ends up with `ERROR`, the model instance is undeployable on VDP. Please refer to [Prepare Models](/docs/prepare-models/overview) to make your model VDP-ready.

<p align="center">
  <img
    src="/docs-assets/core-concepts/model-instance-state.svg"
    alt="model instance state"
    width="500"
  />
  The finite-state-machine (FSM) diagram for the model instance state transition
  logic.
</p>

## Inference

An **inference** is a prediction to a question or task. In the concept of Machine Learning (ML) and Artificial Intelligence (AI), the term inference is often compared with training.
To put it simple, inference is where capabilities learnt during training are put to analyze data to "infer" a result. Inference can be found and are applied everywhere across industries from photo tagging to autonomous driving.

_VDP provides automatic model inference server._
After importing a model into VDP and deploying its model instance online, VDP dynamically generate dedicated API endpoints for _model testing and debugging_.
You can then build end-to-end data pipelines using the models to run ETL operations.

The API supports batch processing: you can send multiple images of popular formats (PNG and JPEG) in one request. Check the examples below.
The API accepts images

- sent by remote URL and Base64 or
- uploaded by multipart.

<CH.Code>

```bash remote
# Test model to infer images with remote URL
curl -X POST \
--header 'Content-Type: application/json' \
--data-raw '{
    "inputs": [
        {
            "image_url": "https://artifacts.instill.tech/dog.jpg"
        },
        {
            "image_url": "https://artifacts.instill.tech/horse.jpg"
        }
    ]
}' \
http://localhost:8083/v1alpha/models/{model-id}/instances/{instance-id}:test
```

```bash base64
# Test model to infer Base64 encoded images
curl -X POST \
--header 'Content-Type: application/json' \
--data-raw '{
    "inputs": [
        {
            "image_base64": "/9j/4AAQSkZ...iiigD/2Q=="
        },
        {
            "image_base64": "/9j/4QBLRXh...UR5+f/2Q=="
        }
    ]
}' \
http://localhost:8083/v1alpha/models/{model-id}/instances/{instance-id}:test
```

```bash multipart
# Test model to infer local images
curl -X POST \
--form 'file=@"dog.jpg"' \
--form 'file=@"horse.jpg"' \
http://localhost:8083/v1alpha/models/{model-id}/instances/{instance-id}:test-multipart
```

</CH.Code>

In which `http://localhost:8083` is the default URL of the VDP model backend. `{model-id}` and `{instance-id}` correspond to the ID of model and model instance respectively.
