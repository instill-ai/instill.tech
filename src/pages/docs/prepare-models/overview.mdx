import { DocsLayout } from "@/components/docs";
export const meta = {
  title: "Prepare Models Overview",
  lang: "en-US",
  draft: false,
  description:
    "Learn about how to prepare your models for the open-source visual data ETL tool VDP https://github.com/instill-ai/vdp",
  date: "",
};

export default ({ children }) => (
  <DocsLayout meta={meta}>{children}</DocsLayout>
);

**Model** is the pipeline component used to process ingested visual data. VDP uses [Triton Inference server](https://github.com/triton-inference-server/server/releases/tag/v2.18.0) for model serving. It supports multiple deep learning frameworks including TensorFlow, PyTorch, TensorRT, ONNX and OpenVINO.
Besides, the [Triton Python Backend](https://github.com/triton-inference-server/python_backend) enables Triton to support any model written in Python.

## VDP model layout

To deploy a model on VDP, we suggest you to prepare the model files following the layout:

```shellscript
â”œâ”€â”€ README.md
â”œâ”€â”€ <pre-model>
â”‚Â Â  â”œâ”€â”€ 1
â”‚Â Â  â”‚Â Â  â””â”€â”€ model.py
â”‚Â Â  â””â”€â”€ config.pbtxt
â”œâ”€â”€ <infer-model>
â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â””â”€â”€ <model-file>
â”‚   â””â”€â”€ config.pbtxt
â”œâ”€â”€ <post-model>
â”‚Â Â  â”œâ”€â”€ 1
â”‚Â Â  â”‚Â Â  â””â”€â”€ model.pyÂ 
â”‚Â Â  â””â”€â”€ config.pbtxt
â””â”€â”€ <ensemble-model>
 Â Â  â”œâ”€â”€ 1
 Â Â  â”‚Â Â  â””â”€â”€ .keep
 Â Â  â””â”€â”€ config.pbtxt
```

The above layout displays a typical VDP model consisting of

- `README.md` - model card to embed the metadata in front matter and descriptions in Markdown format
- `<pre-model>` - Python model to pre-process input images
- `<infer-model>` - Model to convert the unstructured visual data into structured data output, usually a Deep Learning (DL) / Machine Learning (ML) model
- `<post-model>` - Python model to post-process the output of the `infer-model` into desired formats
- `<ensemble-model>` - [Triton ensemble model](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models) to connect the input and output tensors between the pre-processing, inference and post-processing models.
- `config.pbtxt` - [Model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) for each sub model

You can name `<pre-model>`, `<infer-model>`, `<post-model>` and `<ensemble-model>` folders freely provided that the folder names are clear and semantic. All these models bundle into a deployable model for VDP.

> As long as your model fulfils the required [Triton model repository layout](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md), it can be safely imported into VDP and deployed online.

## Serve models written in Python

To deploy your pre-processing and post-processing models with Python code, use [Triton Python Backend](https://github.com/triton-inference-server/python_backend) that supports `conda-pack` to deploy Python models with dependencies.
We have prepared a [custom Conda environment](https://github.com/instill-ai/triton-python-model) with pre-installed libraries including
[scikit-learn](https://github.com/scikit-learn/scikit-learn), [Pillow](https://github.com/python-pillow/Pillow), [PyTorch](https://github.com/pytorch/pytorch), [torchvision](https://pytorch.org/vision/stable/index.html), [Transformers](https://github.com/huggingface/transformers) and [triton_python_model](https://github.com/instill-ai/triton-python-model).
It is shipped with the [NVIDIA GPU Cloud](https://ngc.nvidia.com/) containers using Python 3.8.

If your model is not compatible with Python 3.8 or if it requires additional dependencies, you could [create your own Conda environment](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) and configure the `config.pbtext` to point to the custom conda-pack tar file accordingly.

## Prepare your model to be VDP compatible

- Create a [model card](model-card) `README.md` to describe your model
- Write a [pre-processing model](pre-processing) and a [post-processing model](post-processing) that are compatible with the Triton Python Backend
- Prepare the model configuration file for your inference model
- Set up an [ensemble model](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models) to encapsulate a `pre-processing model â†’ inference model â†’ post-processing model` procedure
- Organise the model files into [valid VDP model layout](#vdp-model-layout)

ðŸ™Œ After preparing your model to be VDP compatible, check out [Import Models](/docs/import-models/overview) to learn about how to import the model into VDP from different sources.
